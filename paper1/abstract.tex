% !TEX root = ../main.tex

\begin{abstract}

Recent years have seen remarkable advances in visual understanding.
However, how to understand a story-based long video with artistic style remains challenging.
In this paper, we introduce MovieNet, a holistic dataset for movie understanding, which contains a large amount of multi-modal data including $1,100$ movies, $33K$ trailers, $3.9M$ photos, $40K$ plot description, \etc.
Besides, different aspects of manual annotations are provided in MovieNet, including $1.1M$ persons with bounding boxes and identities, $42K$ scene boundaries, $2.5K$ aligned description sentences, $65K$ tags of place and action, and $92K$ tags of cinematic style.
To the best of our knowledge, MovieNet is the largest dataset with richest annotations for story-based video understanding.
Based on MovieNet, we set up several benchmarks for movie understanding from different angles.
Extensive experiments are executed on these benchmarks to show the immeasurable value of MovieNet and the distance between current approaches toward comprehensive movie understanding. We believe that such a holistic dataset would promote the researches on story-based video understanding and beyond.
%
\footnote{MovieNet will be published with links to the movie in compliance with regulations. 
All annotations, intermediate features, models and related codes will be released.}
%
\end{abstract}
